
\section{Conclusions}
\label{sec:conclusion}

In this work, we examined the use of concurrency control mechanisms in
a set of 67 open source Ruby on Rails applications and, to a less
thorough extent, concurrency control support in a range of other web
frameworks. We found that, in contrast with traditional transaction
processing, these applications overwhelmingly prefer to leverage
application-level \textit{feral} support for data integrity, typically
in the form of declarative (sometimes user-defined) validation and
association logic. Despite the popularity of these validations, we
find limited use of in-database support to correctly implement them,
leading to a range of quantifiable inconsistencies for Rails' built-in
uniqueness and association validations. While indeed many validations
are invariant confluent and therefore correct under concurrent
execution given standard RDBMS weak isolation and concurrent update
semantics, we see considerable opportunity to better support these
users and their feral validations in the future.


\subsection*{Coda: A Call for Empiricism}

This work is a first step towards better understanding how users in
the wild actually interact with the database systems that this
community builds. Given the ascendancy of open source, there is
unprecedented opportunity to empirically and quantitatively study how
our systems are and are not serving the needs of application
programmers. Lightweight program analysis has never been easier, and
the corpus of readily-accessible code---especially in an academic
context---has never been larger.

Undoubtedly, these open source applications are dwarfed by many other
commercial and enterprise-grade codebases in terms of size, quality,
and complexity. However, compared to alternatives like TPC-C, which
today is almost 23 years old and is still the preferred benchmark for
transaction processing evaluation, open source corpuses are far better
proxies for modern applications (say, written more than a year after
the creation of the World Wide Web). Recent efforts like the
OLTPBenchmark suite~\cite{oltpbench} are a promising step forwards but
are nevertheless (and necessarily) not a substitute for real
application code. The opportunity to perform both quantitative surveys
across a large set of applications as well as longitudinal studies
over the history of each application repository (and the behavior of a
given programmer over time and across repositories) are particularly
compelling. While these studies are inherently imprecise (due to
limitations of the corpus), the resulting quantitative trends are
invaluable.

In summary, in this era of ``Big Data'' analytics, we see great
promise in turning these analyses inwards, towards an empirical
understanding of the usage of database systems today, in service of
better problem selection and a more quantitatively informed community
dialogue.

